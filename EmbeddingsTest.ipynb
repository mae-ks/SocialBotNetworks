{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d55cbb02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import gensim.downloader as gensim_downloader\n",
    "import logging\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69b835d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nbr Sentences: 7752\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    emma = nltk.corpus.gutenberg.sents(nltk.corpus.gutenberg.fileids()[0])\n",
    "    print(\"Nbr Sentences:\", len(emma))\n",
    "except LookupError:\n",
    "    nltk.download('gutenberg')\n",
    "    nltk.download('punkt')\n",
    "    emma = nltk.corpus.gutenberg.sents(nltk.corpus.gutenberg.fileids()[0])\n",
    "    print(\"Nbr Sentences:\", len(emma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "142afbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "emma = [[word.lower() for word in sent if word not in string.punctuation] for sent in emma]\n",
    "emma_joined_sents = [' '.join([word for word in sent]) for sent in emma]\n",
    "emma_joined_sents = [sent for sent in emma_joined_sents if sent != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5af47d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 Samples from Vocabulary:\n",
      " ['yards' 'ye' 'year' 'years' 'yellow' 'yeomanry' 'yes' 'yesterday' 'yet'\n",
      " 'yield' 'yielded' 'yielding' 'york' 'yorkshire' 'you' 'young' 'younger'\n",
      " 'youngest' 'your' 'yours' 'yourself' 'youth' 'youthful' 'zeal' 'zigzags']\n",
      "Nbr Sentences: 7721 Nbr Words: 7239\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "emma_count_vectors = vectorizer.fit_transform(emma_joined_sents)\n",
    "print(\"25 Samples from Vocabulary:\\n\", vectorizer.get_feature_names_out()[-25:])\n",
    "print(f\"Nbr Sentences: {emma_count_vectors.shape[0]} Nbr Words: {emma_count_vectors.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8eb88a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-13 00:11:40,021 : INFO : collecting all words and their counts\n",
      "2021-11-13 00:11:40,022 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2021-11-13 00:11:40,066 : INFO : collected 7322 word types from a corpus of 167069 raw words and 7752 sentences\n",
      "2021-11-13 00:11:40,068 : INFO : Creating a fresh vocabulary\n",
      "2021-11-13 00:11:40,089 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=2 retains 4435 unique words (60.570882272603114%% of original 7322, drops 2887)', 'datetime': '2021-11-13T00:11:40.089544', 'gensim': '4.1.2', 'python': '3.9.6 (tags/v3.9.6:db3ff76, Jun 28 2021, 15:26:21) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'prepare_vocab'}\n",
      "2021-11-13 00:11:40,091 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=2 leaves 164182 word corpus (98.2719714608934%% of original 167069, drops 2887)', 'datetime': '2021-11-13T00:11:40.091544', 'gensim': '4.1.2', 'python': '3.9.6 (tags/v3.9.6:db3ff76, Jun 28 2021, 15:26:21) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'prepare_vocab'}\n",
      "2021-11-13 00:11:40,131 : INFO : deleting the raw counts dictionary of 7322 items\n",
      "2021-11-13 00:11:40,133 : INFO : sample=0.001 downsamples 69 most-common words\n",
      "2021-11-13 00:11:40,134 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 116272.0408529602 word corpus (70.8%% of prior 164182)', 'datetime': '2021-11-13T00:11:40.134425', 'gensim': '4.1.2', 'python': '3.9.6 (tags/v3.9.6:db3ff76, Jun 28 2021, 15:26:21) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'prepare_vocab'}\n",
      "2021-11-13 00:11:40,202 : INFO : estimated required memory for 4435 words and 100 dimensions: 5765500 bytes\n",
      "2021-11-13 00:11:40,204 : INFO : resetting layer weights\n",
      "2021-11-13 00:11:40,208 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2021-11-13T00:11:40.208227', 'gensim': '4.1.2', 'python': '3.9.6 (tags/v3.9.6:db3ff76, Jun 28 2021, 15:26:21) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'build_vocab'}\n",
      "2021-11-13 00:11:40,210 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 4435 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2021-11-13T00:11:40.210223', 'gensim': '4.1.2', 'python': '3.9.6 (tags/v3.9.6:db3ff76, Jun 28 2021, 15:26:21) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'train'}\n",
      "2021-11-13 00:11:40,330 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-11-13 00:11:40,340 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-11-13 00:11:40,343 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-11-13 00:11:40,344 : INFO : EPOCH - 1 : training on 167069 raw words (115990 effective words) took 0.1s, 916800 effective words/s\n",
      "2021-11-13 00:11:40,470 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-11-13 00:11:40,475 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-11-13 00:11:40,481 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-11-13 00:11:40,483 : INFO : EPOCH - 2 : training on 167069 raw words (116296 effective words) took 0.1s, 898943 effective words/s\n",
      "2021-11-13 00:11:40,600 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-11-13 00:11:40,603 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-11-13 00:11:40,610 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-11-13 00:11:40,612 : INFO : EPOCH - 3 : training on 167069 raw words (116394 effective words) took 0.1s, 955913 effective words/s\n",
      "2021-11-13 00:11:40,728 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-11-13 00:11:40,732 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-11-13 00:11:40,734 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-11-13 00:11:40,736 : INFO : EPOCH - 4 : training on 167069 raw words (116204 effective words) took 0.1s, 1018494 effective words/s\n",
      "2021-11-13 00:11:40,845 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-11-13 00:11:40,855 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-11-13 00:11:40,857 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-11-13 00:11:40,858 : INFO : EPOCH - 5 : training on 167069 raw words (116522 effective words) took 0.1s, 999824 effective words/s\n",
      "2021-11-13 00:11:40,860 : INFO : Word2Vec lifecycle event {'msg': 'training on 835345 raw words (581406 effective words) took 0.6s, 896265 effective words/s', 'datetime': '2021-11-13T00:11:40.860845', 'gensim': '4.1.2', 'python': '3.9.6 (tags/v3.9.6:db3ff76, Jun 28 2021, 15:26:21) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'train'}\n",
      "2021-11-13 00:11:40,861 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec(vocab=4435, vector_size=100, alpha=0.025)', 'datetime': '2021-11-13T00:11:40.861845', 'gensim': '4.1.2', 'python': '3.9.6 (tags/v3.9.6:db3ff76, Jun 28 2021, 15:26:21) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'created'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 Samples from Vocabulary:\n",
      "['eyebrows', 'presents', ':\"--', 'sparkling', 'lengths', 'fairest', 'stout', 'falsehood', '_own_', '_home_', 'hereabouts', 'needs', 'faster', 'respecting', 'admissible', 'displayed', 'contradict', 'likenesses', 'eleven', 'faithful', 'augur', 'wore', '?\\'\"', 'unconscious', 'poultry']\n",
      "Most similar words to 'house':\n",
      "[('room', 0.9979537129402161), ('wife', 0.997846782207489), ('family', 0.9972323775291443), ('part', 0.9971916079521179), ('mind', 0.9970117211341858), ('subject', 0.996946394443512), ('first', 0.9965338110923767), ('brought', 0.9964548349380493), ('visit', 0.996389627456665), ('having', 0.9963793158531189)]\n",
      "Word2Vec Embedding for 'house':\n",
      "[-0.29812935  0.2564647   0.1489685   0.20134875 -0.09541447 -0.55086327\n",
      "  0.2629776   0.4896287  -0.37229693 -0.14831354 -0.00734899 -0.6023056\n",
      "  0.2280103  -0.08894787  0.2480529  -0.4374347   0.18319055 -0.25876698\n",
      " -0.26572463 -0.4848458   0.19794495  0.05376895  0.4569192  -0.2980309\n",
      " -0.33282393  0.11609851 -0.09731898 -0.05730578 -0.3100875  -0.10278253\n",
      "  0.41344574 -0.19082761  0.05552464 -0.53513557 -0.06664214  0.6159684\n",
      "  0.08004341 -0.13613673 -0.35291836 -0.3677535   0.1403012  -0.37843043\n",
      " -0.28672618  0.1102052   0.23164794 -0.15457316 -0.3978654  -0.26584807\n",
      "  0.21795146  0.15988366  0.26530263 -0.5119904  -0.20469366 -0.05543342\n",
      " -0.27912983  0.16660424  0.24835283  0.19316678 -0.6346812   0.22707482\n",
      "  0.15244307 -0.02395787  0.0760319  -0.05054167 -0.4370254   0.4091127\n",
      " -0.19340003  0.131598   -0.3187889   0.31438527 -0.33640698  0.19259661\n",
      "  0.32104456 -0.01657418  0.30647913  0.06902171 -0.0810746   0.02937313\n",
      " -0.28193423  0.12060204 -0.34963855 -0.06698049 -0.35833752  0.34328654\n",
      " -0.38333896 -0.15679558  0.07335898  0.23894311  0.28558987  0.08304533\n",
      "  0.24715415  0.10165126  0.12161849 -0.07466504  0.6409811   0.18557438\n",
      "  0.43601218 -0.31846282  0.02942431 -0.3853443 ]\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level = logging.INFO)\n",
    "w2v_model = Word2Vec(emma, vector_size=100, min_count=2)\n",
    "print(f\"25 Samples from Vocabulary:\\n{list(w2v_model.wv.key_to_index.keys())[-25:]}\")\n",
    "print(f\"Most similar words to 'house':\\n{w2v_model.wv.most_similar(['house'])}\")\n",
    "print(f\"Word2Vec Embedding for 'house':\\n{w2v_model.wv['house']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82aa983c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened BERT Embedding:\n",
      "{np.mean(bert_embedding_layer, axis=1}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "input_ids = tf.constant(tokenizer.encode(emma_joined_sents[0]))[None, :]\n",
    "bert_embedding_layer = model(input_ids)[0]\n",
    "print(\"Flattened BERT Embedding:\\n{np.mean(bert_embedding_layer, axis=1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e171708",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentences, tokenizer):\n",
    "    input_ids, input_masks, input_segments = [],[],[]\n",
    "    for sentence in tqdm(sentences):\n",
    "        inputs = tokenizer.encode_plus(\n",
    "            sentence,\n",
    "            add_special_tokens=True,\n",
    "            max_length=128,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            returntoken_type_ids=True\n",
    "        )\n",
    "        \n",
    "        input_ids.append(inputs['input_ids'])\n",
    "        input_masks.append(inputs['attention_mask'])\n",
    "        input_segments.append(inputs['token_type_ids'])\n",
    "    return (\n",
    "        np.asarray(input_ids, dtype='int32'),\n",
    "        np.asarray(input_masks, dtype='int32'),\n",
    "        np.asarray(input_segments, dtype='int32')\n",
    "    )\n",
    "tokens, masks, segments = tokenize(sentences=emma_joined_sents, tokenizer=tokenizer)\n",
    "bert_embedding_layer = model(tokens)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c623665",
   "metadata": {},
   "source": [
    "Linear Regression - Don't overfit polynomial\n",
    "Logistic Regression - Binary classification (using sigmoid function)\n",
    "Feature engineering\n",
    "Multi layer perception"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
